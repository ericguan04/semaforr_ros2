{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Import necessary libraries such as torch and any other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from gst_updated.src.gumbel_social_transformer.st_model import st_model\n",
    "from os.path import join, isdir\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Raw Data\n",
    "Represent raw data as a list:\n",
    "* Each element is a separate **environment**. \n",
    "* There is the **true environment** we get from the real world. \n",
    "* **Other environments** can be made by slightly modifying the true values. This increases the diversity of our outputs to make more accurate trajectory predictions.\n",
    "\n",
    "Represent Pedestrians as a dict:\n",
    "* In each environment, we have a dictionary for each pedestrian identified.\n",
    "* The `ID` key identifies the pedestrian\n",
    "* The `coords` key identifies a list of (x,y) coordinates of the pedestrians path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_traj: tensor([[[[1.0000, 2.0000],\n",
      "          [1.1000, 2.1000],\n",
      "          [1.2000, 2.2000]],\n",
      "\n",
      "         [[2.5000, 3.5000],\n",
      "          [2.6000, 3.6000],\n",
      "          [2.7000, 3.7000]]],\n",
      "\n",
      "\n",
      "        [[[1.2000, 2.1000],\n",
      "          [1.3000, 2.2000],\n",
      "          [1.4000, 2.3000]],\n",
      "\n",
      "         [[2.6000, 3.6000],\n",
      "          [2.7000, 3.7000],\n",
      "          [2.8000, 3.8000]]],\n",
      "\n",
      "\n",
      "        [[[1.1000, 2.2000],\n",
      "          [1.2000, 2.3000],\n",
      "          [1.3000, 2.4000]],\n",
      "\n",
      "         [[2.7000, 3.7000],\n",
      "          [2.8000, 3.8000],\n",
      "          [2.9000, 3.9000]]],\n",
      "\n",
      "\n",
      "        [[[1.3000, 2.3000],\n",
      "          [1.4000, 2.4000],\n",
      "          [1.5000, 2.5000]],\n",
      "\n",
      "         [[2.8000, 3.8000],\n",
      "          [2.9000, 3.9000],\n",
      "          [3.0000, 4.0000]]]])\n",
      "input_binary_mask: tensor([[[[1.],\n",
      "          [1.],\n",
      "          [1.]],\n",
      "\n",
      "         [[1.],\n",
      "          [1.],\n",
      "          [1.]]],\n",
      "\n",
      "\n",
      "        [[[1.],\n",
      "          [1.],\n",
      "          [1.]],\n",
      "\n",
      "         [[1.],\n",
      "          [1.],\n",
      "          [1.]]],\n",
      "\n",
      "\n",
      "        [[[1.],\n",
      "          [1.],\n",
      "          [1.]],\n",
      "\n",
      "         [[1.],\n",
      "          [1.],\n",
      "          [1.]]],\n",
      "\n",
      "\n",
      "        [[[1.],\n",
      "          [1.],\n",
      "          [1.]],\n",
      "\n",
      "         [[1.],\n",
      "          [1.],\n",
      "          [1.]]]])\n",
      "ID to index mapping: {'A': 0, 'B': 1}\n",
      "torch.Size([4, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example raw data with multiple time steps for each environment\n",
    "raw_data = [\n",
    "    # Env 0\n",
    "    [\n",
    "        {'id': 'A', 'coords': [(1.0, 2.0), (1.1, 2.1), (1.2, 2.2)]},\n",
    "        {'id': 'B', 'coords': [(2.5, 3.5), (2.6, 3.6), (2.7, 3.7)]}\n",
    "    ],\n",
    "    # Env 1\n",
    "    [\n",
    "        {'id': 'A', 'coords': [(1.2, 2.1), (1.3, 2.2), (1.4, 2.3)]},\n",
    "        {'id': 'B', 'coords': [(2.6, 3.6), (2.7, 3.7), (2.8, 3.8)]}\n",
    "    ],\n",
    "    # Env 2\n",
    "    [\n",
    "        {'id': 'A', 'coords': [(1.1, 2.2), (1.2, 2.3), (1.3, 2.4)]},\n",
    "        {'id': 'B', 'coords': [(2.7, 3.7), (2.8, 3.8), (2.9, 3.9)]}\n",
    "    ],\n",
    "    # Env 3\n",
    "    [\n",
    "        {'id': 'A', 'coords': [(1.3, 2.3), (1.4, 2.4), (1.5, 2.5)]},\n",
    "        {'id': 'B', 'coords': [(2.8, 3.8), (2.9, 3.9), (3.0, 4.0)]}\n",
    "    ]\n",
    "]\n",
    "\n",
    "# PREPROCESSING STEPS\n",
    "\n",
    "# Step 1: Create ID to index mapping (Assuming IDs are unique across all environments)\n",
    "unique_ids = sorted(set(person['id'] for env in raw_data for person in env))\n",
    "id_to_index = {id_: idx for idx, id_ in enumerate(unique_ids)}\n",
    "\n",
    "# Initialize tensor dimensions\n",
    "number_of_env = len(raw_data)\n",
    "number_of_pedestrians = len(unique_ids)  # Total unique pedestrians\n",
    "number_of_time_steps = len(raw_data[0][0]['coords'])  # Time steps per pedestrian\n",
    "\n",
    "# Initialize tensors\n",
    "input_traj = torch.zeros((number_of_env, number_of_pedestrians, number_of_time_steps, 2))\n",
    "input_binary_mask = torch.zeros((number_of_env, number_of_pedestrians, number_of_time_steps, 1))\n",
    "\n",
    "# Step 2: Fill tensors based on raw data using the ID mapping\n",
    "for env_idx, env_data in enumerate(raw_data):\n",
    "    for person in env_data:\n",
    "        id_ = person['id']\n",
    "        coords = person['coords']\n",
    "        \n",
    "        # Find index using the ID mapping\n",
    "        ped_idx = id_to_index[id_]\n",
    "        \n",
    "        # Fill in the coordinates and set the binary mask to 1.0 for this person at each time step\n",
    "        for t, coord in enumerate(coords):\n",
    "            input_traj[env_idx, ped_idx, t, :2] = torch.tensor(coord)\n",
    "            input_binary_mask[env_idx, ped_idx, t, 0] = 1.0\n",
    "\n",
    "# Check results\n",
    "print(\"input_traj:\", input_traj)\n",
    "print(\"input_binary_mask:\", input_binary_mask)\n",
    "print(\"ID to index mapping:\", id_to_index)\n",
    "print(input_traj.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Functions for Preprocessing\n",
    "* `seq_to_graph()` converts the processed tensor into a representation ready for input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_graph(seq_, seq_rel, attn_mech='rel_conv'):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - seq_ # (n_env, num_peds, 2, obs_seq_len)\n",
    "        - seq_rel # (n_env, num_peds, 2, obs_seq_len)\n",
    "    outputs:\n",
    "        - V # (n_env, obs_seq_len, num_peds, 2)\n",
    "        - A # (n_env, obs_seq_len, num_peds, num_peds, 2)\n",
    "    \"\"\"\n",
    "    V = seq_rel.permute(0, 3, 1, 2) # (n_env, obs_seq_len, num_peds, 2)\n",
    "    seq_permute = seq_.permute(0, 3, 1, 2) # (n_env, obs_seq_len, num_peds, 2)\n",
    "    A = seq_permute.unsqueeze(3)-seq_permute.unsqueeze(2) # (n_env, obs_seq_len, num_peds, 1, 2) - (n_env, obs_seq_len, 1, num_peds, 2)\n",
    "    return V, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper / Pipeline Class declaration\n",
    "* This wraps the entire pipeline together\n",
    "* Loads the model from weights and defines the forward function\n",
    "* The forward function will give the output trajectories\n",
    "* Input trajectory and input binary mask need to be in the proper format from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdNavPredInterfaceMultiEnv(object):\n",
    "\n",
    "    def __init__(self, load_path, device, config, num_env):\n",
    "        # *** Load model\n",
    "        self.args = config\n",
    "        self.device = device\n",
    "        self.nenv = num_env\n",
    "\n",
    "        self.args_eval = config\n",
    "        checkpoint_dir = join(load_path, 'checkpoint')\n",
    "        self.model = st_model(self.args_eval, device=device).to(device)\n",
    "        model_filename = 'epoch_'+str(self.args_eval.num_epochs)+'.pt'\n",
    "        model_checkpoint = torch.load(join(checkpoint_dir, model_filename), map_location=device)\n",
    "        self.model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        print(\"LOADED MODEL\")\n",
    "        print(\"device: \", device)\n",
    "        print()\n",
    "\n",
    "    def forward(self, input_traj,input_binary_mask, sampling = True):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            - input_traj:\n",
    "                # numpy\n",
    "                # (n_env, num_peds, obs_seq_len, 2)\n",
    "            - input_binary_mask:\n",
    "                # numpy\n",
    "                # (n_env, num_peds, obs_seq_len, 1)\n",
    "                # Zhe: I think we should not just have the binary mask of shape (n_env, number of pedestrains, 1)\n",
    "                # because some agents are partially detected, and they should not be simply ignored.\n",
    "            - sampling:\n",
    "                # bool\n",
    "                # True means you sample from Gaussian.\n",
    "                # False means you choose to use the mean of Gaussian as output.\n",
    "        outputs:\n",
    "            - output_traj:\n",
    "                # torch \"cpu\"\n",
    "                # (n_env, num_peds, pred_seq_len, 5)\n",
    "                # where 5 includes [mu_x, mu_y, sigma_x, sigma_y, correlation coefficient]\n",
    "            - output_binary_mask:\n",
    "                # torch \"cpu\"\n",
    "                # (n_env, num_peds, 1)\n",
    "                # Zhe: this means for prediction, if an agent does not show up in the last and second\n",
    "                # last observation time step, then the agent will not be predicted.\n",
    "        \"\"\"\n",
    "\n",
    "        invalid_value = -999.\n",
    "        # *** Process input data\n",
    "        obs_traj = input_traj.permute(0,1,3,2) # (n_env, num_peds, 2, obs_seq_len)\n",
    "        n_env, num_peds = obs_traj.shape[:2]\n",
    "        loss_mask_obs = input_binary_mask[:,:,:,0] # (n_env, num_peds, obs_seq_len)\n",
    "        loss_mask_rel_obs = loss_mask_obs[:,:,:-1] * loss_mask_obs[:,:,-1:]\n",
    "        loss_mask_rel_obs = torch.cat((loss_mask_obs[:,:,:1], loss_mask_rel_obs), dim=2) # (n_env, num_peds, obs_seq_len)\n",
    "        loss_mask_rel_pred = (torch.ones((n_env, num_peds, self.args_eval.pred_seq_len), device=self.device) * loss_mask_rel_obs[:,:,-1:])\n",
    "        loss_mask_rel = torch.cat((loss_mask_rel_obs, loss_mask_rel_pred), dim=2) # (n_env, num_peds, seq_len)\n",
    "        loss_mask_pred = loss_mask_rel_pred\n",
    "        loss_mask_rel_obs_permute = loss_mask_rel_obs.permute(0,2,1).reshape(n_env*self.args_eval.obs_seq_len, num_peds) # (n_env*obs_seq_len, num_peds)\n",
    "        attn_mask_obs = torch.bmm(loss_mask_rel_obs_permute.unsqueeze(2), loss_mask_rel_obs_permute.unsqueeze(1)) # (n_env*obs_seq_len, num_peds, num_peds)\n",
    "        attn_mask_obs = attn_mask_obs.reshape(n_env, self.args_eval.obs_seq_len, num_peds, num_peds)\n",
    "        \n",
    "        obs_traj_rel = obs_traj[:,:,:,1:] - obs_traj[:,:,:,:-1]\n",
    "        obs_traj_rel = torch.cat((torch.zeros(n_env, num_peds, 2, 1, device=self.device), obs_traj_rel), dim=3)\n",
    "        obs_traj_rel = invalid_value*torch.ones_like(obs_traj_rel)*(1-loss_mask_rel_obs.unsqueeze(2)) \\\n",
    "            + obs_traj_rel*loss_mask_rel_obs.unsqueeze(2)\n",
    "        v_obs, A_obs = seq_to_graph(obs_traj, obs_traj_rel, attn_mech='rel_conv')\n",
    "        # *** Perform trajectory prediction\n",
    "        sampling = False\n",
    "        with torch.no_grad():\n",
    "            v_obs, A_obs, attn_mask_obs, loss_mask_rel = \\\n",
    "                v_obs.to(self.device), A_obs.to(self.device), attn_mask_obs.to(self.device), loss_mask_rel.to(self.device)\n",
    "            results = self.model(v_obs, A_obs, attn_mask_obs, loss_mask_rel, tau=0.03, hard=True, sampling=sampling, device=self.device)\n",
    "            gaussian_params_pred, x_sample_pred, info = results\n",
    "        mu, sx, sy, corr = gaussian_params_pred\n",
    "        mu = mu.cumsum(1)\n",
    "        sx_squared = sx**2.\n",
    "        sy_squared = sy**2.\n",
    "        corr_sx_sy = corr*sx*sy\n",
    "        sx_squared_cumsum = sx_squared.cumsum(1)\n",
    "        sy_squared_cumsum = sy_squared.cumsum(1)\n",
    "        corr_sx_sy_cumsum = corr_sx_sy.cumsum(1)\n",
    "        sx_cumsum = sx_squared_cumsum**(1./2)\n",
    "        sy_cumsum = sy_squared_cumsum**(1./2)\n",
    "        corr_cumsum = corr_sx_sy_cumsum/(sx_cumsum*sy_cumsum)\n",
    "        mu_cumsum = mu.detach().to(self.device) + obs_traj.permute(0,3,1,2)[:,-1:]# np.transpose(obs_traj[:,:,:,-1:], (0,3,1,2)) # (batch, time, node, 2)\n",
    "        mu_cumsum = mu_cumsum * loss_mask_pred.permute(0,2,1).unsqueeze(-1) + invalid_value*(1-loss_mask_pred.permute(0,2,1).unsqueeze(-1))\n",
    "        output_traj = torch.cat((mu_cumsum.detach().to(self.device), sx_cumsum.detach().to(self.device), sy_cumsum.detach().to(self.device), corr_cumsum.detach().to(self.device)), dim=3)\n",
    "        output_traj = output_traj.permute(0, 2, 1, 3) # (n_env, num_peds, pred_seq_len, 5)\n",
    "        output_binary_mask = loss_mask_pred[:,:,:1].detach().to(self.device) # (n_env, num_peds, 1) # first step same as following in prediction\n",
    "        return output_traj, output_binary_mask\n",
    "\n",
    "\n",
    "def visualize_output_trajectory_deterministic(input_traj, input_binary_mask, output_traj, output_binary_mask, sample_index, obs_seq_len=5, pred_seq_len=5):\n",
    "    ##### Print Visualization Started #####\n",
    "    input_traj_i = input_traj[sample_index]\n",
    "    input_binary_mask_i = input_binary_mask[sample_index]\n",
    "    output_traj_i = output_traj[sample_index]\n",
    "    output_binary_mask_i = output_binary_mask[sample_index]\n",
    "    num_peds, seq_len = input_traj_i.shape[0], obs_seq_len+pred_seq_len\n",
    "    full_obs_ped_idx = np.where(input_binary_mask_i.sum(1)[:,0]==obs_seq_len)[0]\n",
    "    full_traj = np.concatenate((input_traj_i, output_traj_i[:,:,:2]), axis=1)\n",
    "    output_binary_mask_i_pred_len = np.stack([output_binary_mask_i for j in range(pred_seq_len)], axis=1)\n",
    "    loss_mask = np.concatenate((input_binary_mask_i, output_binary_mask_i_pred_len), axis=1)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_tight_layout(True)\n",
    "    for ped_idx in range(num_peds):\n",
    "        if ped_idx in full_obs_ped_idx:\n",
    "            ax.plot(full_traj[ped_idx, obs_seq_len:, 0], full_traj[ped_idx, obs_seq_len:, 1], '.-', c='r')\n",
    "            ax.plot(full_traj[ped_idx, :obs_seq_len, 0], full_traj[ped_idx, :obs_seq_len, 1], '.-', c='k') # black for obs   \n",
    "        else:\n",
    "            for t_idx in range(seq_len):\n",
    "                if loss_mask[ped_idx,t_idx,0] == 1:\n",
    "                    if t_idx < obs_seq_len:\n",
    "                        # obs blue for partially detected pedestrians\n",
    "                        ax.plot(full_traj[ped_idx, t_idx, 0], full_traj[ped_idx, t_idx, 1], '.', c='b')\n",
    "                    else:\n",
    "                        # pred orange for partially detected pedestrians\n",
    "                        ax.plot(full_traj[ped_idx, t_idx, 0], full_traj[ped_idx, t_idx, 1], '.', c='C1', alpha=0.2)\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.plot()\n",
    "    fig.savefig(str(sample_index)+\".png\")\n",
    "    print(str(sample_index)+\".png is created.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT DATA\n",
      "number of environments:  4\n",
      "input_traj shape:  torch.Size([4, 2, 3, 2])\n",
      "input_binary_mask shape: torch.Size([4, 2, 3, 1])\n",
      "\n",
      "Namespace(spatial='gumbel_social_transformer', temporal='faster_lstm', output_dim=5, embedding_size=64, spatial_num_heads=8, lstm_hidden_size=64, lstm_num_layers=1, decode_style='recursive', detach_sample=False, motion_dim=2, dataset='sj', obs_seq_len=3, pred_seq_len=3, rotation_pattern=None, batch_size=64, num_epochs=100, lr=0.001, clip_grad=None, organize_csv=False, spatial_num_layers=1, only_observe_full_period=False, spatial_num_heads_edges=0, ghost=False, random_seed=1000, deterministic=False, init_temp=0.5, resume_training=False, temp_epochs=100, save_epochs=10, resume_epoch=None)\n",
      "No ghost version.\n",
      "new gst\n",
      "new st model\n",
      "LOADED MODEL\n",
      "device:  cpu\n",
      "\n",
      "\n",
      "OUTPUT DATA\n",
      "output_traj shape:  torch.Size([4, 2, 3, 5])\n",
      "output_binary_mask shape: torch.Size([4, 2, 1])\n",
      "\n",
      "tensor([[[[ 1.3341,  2.1802,  0.2916,  0.6168, -0.0849],\n",
      "          [ 1.4686,  2.1453,  0.3265,  0.6800, -0.0937],\n",
      "          [ 1.5887,  2.1260,  0.3426,  0.7019, -0.0971]],\n",
      "\n",
      "         [[ 2.8341,  3.6802,  0.2916,  0.6168, -0.0849],\n",
      "          [ 2.9686,  3.6453,  0.3265,  0.6800, -0.0937],\n",
      "          [ 3.0887,  3.6260,  0.3426,  0.7019, -0.0971]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5341,  2.2802,  0.2916,  0.6168, -0.0849],\n",
      "          [ 1.6686,  2.2453,  0.3265,  0.6800, -0.0937],\n",
      "          [ 1.7887,  2.2260,  0.3426,  0.7019, -0.0971]],\n",
      "\n",
      "         [[ 2.9341,  3.7802,  0.2916,  0.6168, -0.0849],\n",
      "          [ 3.0686,  3.7453,  0.3265,  0.6800, -0.0937],\n",
      "          [ 3.1887,  3.7260,  0.3426,  0.7019, -0.0971]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4341,  2.3802,  0.2916,  0.6168, -0.0849],\n",
      "          [ 1.5686,  2.3453,  0.3265,  0.6800, -0.0937],\n",
      "          [ 1.6887,  2.3260,  0.3426,  0.7019, -0.0971]],\n",
      "\n",
      "         [[ 3.0341,  3.8802,  0.2916,  0.6168, -0.0849],\n",
      "          [ 3.1686,  3.8453,  0.3265,  0.6800, -0.0937],\n",
      "          [ 3.2887,  3.8260,  0.3426,  0.7019, -0.0971]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6341,  2.4802,  0.2916,  0.6168, -0.0849],\n",
      "          [ 1.7686,  2.4453,  0.3265,  0.6800, -0.0937],\n",
      "          [ 1.8887,  2.4260,  0.3426,  0.7019, -0.0971]],\n",
      "\n",
      "         [[ 3.1341,  3.9802,  0.2916,  0.6168, -0.0849],\n",
      "          [ 3.2686,  3.9453,  0.3265,  0.6800, -0.0937],\n",
      "          [ 3.3887,  3.9260,  0.3426,  0.7019, -0.0971]]]])\n",
      "Pedestrian A:\n",
      "  - (1.3341162204742432, 2.1802477836608887)\n",
      "  - (1.4686384201049805, 2.1453216075897217)\n",
      "  - (1.5887095928192139, 2.125965118408203)\n",
      "  - (1.5341161489486694, 2.280247688293457)\n",
      "  - (1.6686382293701172, 2.24532151222229)\n",
      "  - (1.7887091636657715, 2.2259647846221924)\n",
      "  - (1.4341161251068115, 2.3802478313446045)\n",
      "  - (1.5686382055282593, 2.3453216552734375)\n",
      "  - (1.6887092590332031, 2.32596492767334)\n",
      "  - (1.6341161727905273, 2.480247735977173)\n",
      "  - (1.768638253211975, 2.445321559906006)\n",
      "  - (1.8887094259262085, 2.4259650707244873)\n",
      "Pedestrian B:\n",
      "  - (2.8341164588928223, 3.6802477836608887)\n",
      "  - (2.9686386585235596, 3.6453216075897217)\n",
      "  - (3.088709831237793, 3.625965118408203)\n",
      "  - (2.9341161251068115, 3.780247688293457)\n",
      "  - (3.0686380863189697, 3.74532151222229)\n",
      "  - (3.188709259033203, 3.7259650230407715)\n",
      "  - (3.034116506576538, 3.8802475929260254)\n",
      "  - (3.1686387062072754, 3.8453214168548584)\n",
      "  - (3.2887096405029297, 3.825965166091919)\n",
      "  - (3.1341161727905273, 3.980247735977173)\n",
      "  - (3.2686381340026855, 3.945321559906006)\n",
      "  - (3.388709306716919, 3.9259650707244873)\n"
     ]
    }
   ],
   "source": [
    "obs_seq_len = 3\n",
    "pred_seq_len = 3\n",
    "invalid_value = -999.\n",
    "\n",
    "input_trajectory, input_binary_masker = input_traj, input_binary_mask\n",
    "n_env = input_traj.shape[0]\n",
    "assert input_traj.shape[2] == obs_seq_len\n",
    "#print(input_traj)\n",
    "#print(input_binary_mask)\n",
    "#print(n_env)\n",
    "\"\"\"\n",
    "- input_traj:\n",
    "    # tensor\n",
    "    # (n_env, num_peds, obs_seq_len, 2)\n",
    "- input_binary_mask:\n",
    "    # tensor\n",
    "    # (n_env, num_peds, obs_seq_len, 1)\n",
    "\"\"\"\n",
    "print()\n",
    "print(\"INPUT DATA\")\n",
    "print(\"number of environments: \", n_env)\n",
    "print(\"input_traj shape: \", input_traj.shape)\n",
    "print(\"input_binary_mask shape:\", input_binary_mask.shape)\n",
    "print()\n",
    "\n",
    "load_path = '/Users/ericguan/Documents/CrowdNav_Prediction_AttnGraph/gst_updated/results/100-gumbel_social_transformer-faster_lstm-lr_0.001-init_temp_0.5-edge_head_0-ebd_64-snl_1-snh_8-seed_1000/sj'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load args from a pickle file\n",
    "\n",
    "args_path = '/Users/ericguan/Documents/CrowdNav_Prediction_AttnGraph/gst_updated/results/100-gumbel_social_transformer-faster_lstm-lr_0.001-init_temp_0.5-edge_head_0-ebd_64-snl_1-snh_8-seed_1000/sj/checkpoint/args.pickle'\n",
    "with open(args_path, 'rb') as f:\n",
    "    args = pickle.load(f)\n",
    "args.obs_seq_len = 3\n",
    "args.pred_seq_len = 3\n",
    "print(args)\n",
    "model = CrowdNavPredInterfaceMultiEnv(load_path=load_path,\n",
    "                                        device=device, config = args, num_env=n_env)\n",
    "\n",
    "input_traj = input_traj.cpu()\n",
    "input_binary_mask = input_binary_mask.cpu()\n",
    "output_traj, output_binary_mask = model.forward(\n",
    "    input_traj,\n",
    "    input_binary_mask,\n",
    "    sampling = True,\n",
    ")\n",
    "print()\n",
    "print(\"OUTPUT DATA\")\n",
    "print(\"output_traj shape: \", output_traj.shape)\n",
    "print(\"output_binary_mask shape:\", output_binary_mask.shape)\n",
    "print()\n",
    "print(output_traj)\n",
    "# for sample_index in range(n_env):\n",
    "#     visualize_output_trajectory_deterministic(input_traj, input_binary_mask, output_traj, output_binary_mask, sample_index, obs_seq_len=3, pred_seq_len=3)\n",
    "\n",
    "def output_traj_to_dict(output_traj, id_to_index):\n",
    "    \"\"\"\n",
    "    Convert the output trajectory tensor back into a dictionary format.\n",
    "    \n",
    "    Args:\n",
    "    - output_traj (torch.Tensor): The output trajectory tensor of shape (n_env, num_peds, pred_seq_len, 5).\n",
    "    - id_to_index (dict): A dictionary mapping pedestrian IDs to their indices.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are pedestrian IDs and values are lists of (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    index_to_id = {v: k for k, v in id_to_index.items()}\n",
    "    n_env, num_peds, pred_seq_len, _ = output_traj.shape\n",
    "    output_dict = {index_to_id[i]: [] for i in range(num_peds)}\n",
    "\n",
    "    for env_idx in range(n_env):\n",
    "        for ped_idx in range(num_peds):\n",
    "            for t in range(pred_seq_len):\n",
    "                x, y = output_traj[env_idx, ped_idx, t, :2].tolist()\n",
    "                output_dict[index_to_id[ped_idx]].append((x, y))\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Convert the output trajectory tensor back into a dictionary format\n",
    "output_dict = output_traj_to_dict(output_traj, id_to_index)\n",
    "# print(output_dict)\n",
    "\n",
    "for key, value in output_dict.items():\n",
    "    print(f\"Pedestrian {key}:\")\n",
    "    for coord in value:\n",
    "        print(f\"  - {coord}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crowdnav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
